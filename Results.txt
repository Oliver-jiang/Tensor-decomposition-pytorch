                                                                                      DogvsCat

###### VGG16 ######

training (pretrain with imagenet):

    Accuracy : 0.95985
    Average prediction time 0.27738998806665816 63
    inference time: 0:00:19.152431

    Architecture:
    ModifiedVGG16Model(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Dropout(p=0.5, inplace=False)
    (1): Linear(in_features=25088, out_features=4096, bias=True)
    (2): ReLU(inplace=True)
    (3): Dropout(p=0.5, inplace=False)
    (4): Linear(in_features=4096, out_features=4096, bias=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=4096, out_features=2, bias=True)
  )
)
    Parameters:
        module name  input shape output shape       params memory(MB)              MAdd             Flops   MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)
0        features.0    3 224 224   64 224 224       1792.0      12.25     173,408,256.0      89,915,392.0     609280.0   12845056.0       4.87%   13454336.0
1        features.1   64 224 224   64 224 224          0.0      12.25       3,211,264.0       3,211,264.0   12845056.0   12845056.0       0.21%   25690112.0
2        features.2   64 224 224   64 224 224      36928.0      12.25   3,699,376,128.0   1,852,899,328.0   12992768.0   12845056.0       7.82%   25837824.0
3        features.3   64 224 224   64 224 224          0.0      12.25       3,211,264.0       3,211,264.0   12845056.0   12845056.0       0.19%   25690112.0
4        features.4   64 224 224   64 112 112          0.0       3.06       2,408,448.0       3,211,264.0   12845056.0    3211264.0      13.36%   16056320.0
5        features.5   64 112 112  128 112 112      73856.0       6.12   1,849,688,064.0     926,449,664.0    3506688.0    6422528.0       4.05%    9929216.0
6        features.6  128 112 112  128 112 112          0.0       6.12       1,605,632.0       1,605,632.0    6422528.0    6422528.0       0.13%   12845056.0
7        features.7  128 112 112  128 112 112     147584.0       6.12   3,699,376,128.0   1,851,293,696.0    7012864.0    6422528.0       3.90%   13435392.0
8        features.8  128 112 112  128 112 112          0.0       6.12       1,605,632.0       1,605,632.0    6422528.0    6422528.0       0.13%   12845056.0
9        features.9  128 112 112  128  56  56          0.0       1.53       1,204,224.0       1,605,632.0    6422528.0    1605632.0       6.42%    8028160.0
10      features.10  128  56  56  256  56  56     295168.0       3.06   1,849,688,064.0     925,646,848.0    2786304.0    3211264.0       2.07%    5997568.0
11      features.11  256  56  56  256  56  56          0.0       3.06         802,816.0         802,816.0    3211264.0    3211264.0       0.11%    6422528.0
12      features.12  256  56  56  256  56  56     590080.0       3.06   3,699,376,128.0   1,850,490,880.0    5571584.0    3211264.0       3.94%    8782848.0
13      features.13  256  56  56  256  56  56          0.0       3.06         802,816.0         802,816.0    3211264.0    3211264.0       0.10%    6422528.0
14      features.14  256  56  56  256  56  56     590080.0       3.06   3,699,376,128.0   1,850,490,880.0    5571584.0    3211264.0       3.40%    8782848.0
15      features.15  256  56  56  256  56  56          0.0       3.06         802,816.0         802,816.0    3211264.0    3211264.0       0.10%    6422528.0
16      features.16  256  56  56  256  28  28          0.0       0.77         602,112.0         802,816.0    3211264.0     802816.0       2.45%    4014080.0
17      features.17  256  28  28  512  28  28    1180160.0       1.53   1,849,688,064.0     925,245,440.0    5523456.0    1605632.0       1.86%    7129088.0
18      features.18  512  28  28  512  28  28          0.0       1.53         401,408.0         401,408.0    1605632.0    1605632.0       0.10%    3211264.0
19      features.19  512  28  28  512  28  28    2359808.0       1.53   3,699,376,128.0   1,850,089,472.0   11044864.0    1605632.0       2.76%   12650496.0
20      features.20  512  28  28  512  28  28          0.0       1.53         401,408.0         401,408.0    1605632.0    1605632.0       0.08%    3211264.0
21      features.21  512  28  28  512  28  28    2359808.0       1.53   3,699,376,128.0   1,850,089,472.0   11044864.0    1605632.0       2.31%   12650496.0
22      features.22  512  28  28  512  28  28          0.0       1.53         401,408.0         401,408.0    1605632.0    1605632.0       0.07%    3211264.0
23      features.23  512  28  28  512  14  14          0.0       0.38         301,056.0         401,408.0    1605632.0     401408.0       1.28%    2007040.0
24      features.24  512  14  14  512  14  14    2359808.0       0.38     924,844,032.0     462,522,368.0    9840640.0     401408.0       1.14%   10242048.0
25      features.25  512  14  14  512  14  14          0.0       0.38         100,352.0         100,352.0     401408.0     401408.0       0.05%     802816.0
26      features.26  512  14  14  512  14  14    2359808.0       0.38     924,844,032.0     462,522,368.0    9840640.0     401408.0       0.89%   10242048.0
27      features.27  512  14  14  512  14  14          0.0       0.38         100,352.0         100,352.0     401408.0     401408.0       0.05%     802816.0
28      features.28  512  14  14  512  14  14    2359808.0       0.38     924,844,032.0     462,522,368.0    9840640.0     401408.0       0.88%   10242048.0
29      features.29  512  14  14  512  14  14          0.0       0.38         100,352.0         100,352.0     401408.0     401408.0       0.05%     802816.0
30      features.30  512  14  14  512   7   7          0.0       0.10          75,264.0         100,352.0     401408.0     100352.0       0.43%     501760.0
31          avgpool  512   7   7  512   7   7          0.0       0.10               0.0               0.0          0.0          0.0       0.32%          0.0
32     classifier.0        25088        25088          0.0       0.10               0.0               0.0          0.0          0.0       0.04%          0.0
33     classifier.1        25088         4096  102764544.0       0.02     205,516,800.0     102,760,448.0  411158528.0      16384.0      29.32%  411174912.0
34     classifier.2         4096         4096          0.0       0.02           4,096.0           4,096.0      16384.0      16384.0       0.03%      32768.0
35     classifier.3         4096         4096          0.0       0.02               0.0               0.0          0.0          0.0       0.02%          0.0
36     classifier.4         4096         4096   16781312.0       0.02      33,550,336.0      16,777,216.0   67141632.0      16384.0       5.00%   67158016.0
37     classifier.5         4096         4096          0.0       0.02           4,096.0           4,096.0      16384.0      16384.0       0.02%      32768.0
38     classifier.6         4096            2       8194.0       0.00          16,382.0           8,192.0      49160.0          8.0       0.05%      49168.0
total                                          134268738.0     109.47  30,950,491,646.0  15,499,401,216.0      49160.0          8.0     100.00%  766811408.0
============================================================================================================================================================
Total params: 134,268,738
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total memory: 109.47MB
Total MAdd: 30.95GMAdd
Total Flops: 15.5GFlops
Total MemR+W: 731.29MB


Fine-tune (20 epochs):
    Begining: 
        Accuracy : 0.674875
        Average prediction time 0.13521791427854507 63
        inference time: 0:00:11.777571

    Process:
        Accuracy : 0.754875
        Accuracy : 0.814875
        Accuracy : 0.879375
        Accuracy : 0.876375
        Accuracy : 0.886775
        Accuracy : 0.889375
        Accuracy : 0.899375
        Accuracy : 0.908505
        Accuracy : 0.909005
        Accuracy : 0.921875
        Accuracy : 0.931875
        Accuracy : 0.936875
        Accuracy : 0.940875
        Accuracy : 0.944625
        Accuracy : 0.95075
        Accuracy : 0.9375
        Accuracy : 0.956375

    Ending:
        Accuracy : 0.952875
        Average prediction time 0.19401696371653723 63
        inference time: 0:00:13.847323

    Architecture (decomposed):
    (module): ModifiedVGG16Model(
        (features): Sequential(
        (0): Sequential(
            (0): Conv2d(3, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): Conv2d(2, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): Conv2d(14, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): ReLU(inplace=True)
        (2): Sequential(
            (0): Conv2d(64, 29, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): Conv2d(29, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): Conv2d(26, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (3): ReLU(inplace=True)
        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (5): Sequential(
            (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): Conv2d(32, 39, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): Conv2d(39, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (6): ReLU(inplace=True)
        (7): Sequential(
            (0): Conv2d(128, 43, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): Conv2d(43, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): Conv2d(42, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (8): ReLU(inplace=True)
        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (10): Sequential(
            (0): Conv2d(128, 59, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): Conv2d(59, 71, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): Conv2d(71, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (11): ReLU(inplace=True)
        (12): Sequential(
            (0): Conv2d(256, 81, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): Conv2d(81, 78, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): Conv2d(78, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (13): ReLU(inplace=True)
        (14): Sequential(
            (0): Conv2d(256, 73, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): Conv2d(73, 70, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): Conv2d(70, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (15): ReLU(inplace=True)
        (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (17): Sequential(
            (0): Conv2d(256, 110, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): Conv2d(110, 131, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): Conv2d(131, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (18): ReLU(inplace=True)
        (19): Sequential(
            (0): Conv2d(512, 148, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): Conv2d(148, 138, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): Conv2d(138, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (20): ReLU(inplace=True)
        (21): Sequential(
            (0): Conv2d(512, 132, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): Conv2d(132, 121, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): Conv2d(121, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (22): ReLU(inplace=True)
        (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (24): Sequential(
            (0): Conv2d(512, 157, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): Conv2d(157, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): Conv2d(144, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (25): ReLU(inplace=True)
        (26): Sequential(
            (0): Conv2d(512, 149, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): Conv2d(149, 147, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): Conv2d(147, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (27): ReLU(inplace=True)
        (28): Sequential(
            (0): Conv2d(512, 155, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): Conv2d(155, 152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): Conv2d(152, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (29): ReLU(inplace=True)
        (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
        (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
        (classifier): Sequential(
        (0): Dropout(p=0.5, inplace=False)
        (1): Linear(in_features=25088, out_features=4096, bias=True)
        (2): ReLU(inplace=True)
        (3): Dropout(p=0.5, inplace=False)
        (4): Linear(in_features=4096, out_features=4096, bias=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=4096, out_features=2, bias=True)
        )
    )
    )

    Parameters:
         module name  input shape output shape       params memory(MB)             MAdd            Flops   MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)
0       features.0.0    3 224 224    2 224 224          6.0       0.38        501,760.0        301,056.0     602136.0     401408.0       0.45%    1003544.0
1       features.0.1    2 224 224   14 224 224        252.0       2.68     24,586,240.0     12,644,352.0     402416.0    2809856.0       1.67%    3212272.0
2       features.0.2   14 224 224   64 224 224        960.0      12.25     89,915,392.0     48,168,960.0    2813696.0   12845056.0       5.02%   15658752.0
3         features.1   64 224 224   64 224 224          0.0      12.25      3,211,264.0      3,211,264.0   12845056.0   12845056.0       0.30%   25690112.0
4       features.2.0   64 224 224   29 224 224       1856.0       5.55    184,798,208.0     93,126,656.0   12852480.0    5820416.0       4.61%   18672896.0
5       features.2.1   29 224 224   26 224 224       6786.0       4.98    679,684,096.0    340,494,336.0    5847560.0    5218304.0       3.36%   11065864.0
6       features.2.2   26 224 224   64 224 224       1728.0      12.25    166,985,728.0     86,704,128.0    5225216.0   12845056.0       3.96%   18070272.0
7         features.3   64 224 224   64 224 224          0.0      12.25      3,211,264.0      3,211,264.0   12845056.0   12845056.0       0.19%   25690112.0
8         features.4   64 224 224   64 112 112          0.0       3.06      2,408,448.0      3,211,264.0   12845056.0    3211264.0      10.68%   16056320.0
9       features.5.0   64 112 112   32 112 112       2048.0       1.53     50,978,816.0     25,690,112.0    3219456.0    1605632.0       0.96%    4825088.0
10      features.5.1   32 112 112   39 112 112      11232.0       1.87    281,299,200.0    140,894,208.0    1650560.0    1956864.0       3.10%    3607424.0
11      features.5.2   39 112 112  128 112 112       5120.0       6.12    125,239,296.0     64,225,280.0    1977344.0    6422528.0       2.05%    8399872.0
12        features.6  128 112 112  128 112 112          0.0       6.12      1,605,632.0      1,605,632.0    6422528.0    6422528.0       0.11%   12845056.0
13      features.7.0  128 112 112   43 112 112       5504.0       2.06    137,544,960.0     69,042,176.0    6444544.0    2157568.0       1.73%    8602112.0
14      features.7.1   43 112 112   42 112 112      16254.0       2.01    407,253,504.0    203,890,176.0    2222584.0    2107392.0       1.45%    4329976.0
15      features.7.2   42 112 112  128 112 112       5504.0       6.12    134,873,088.0     69,042,176.0    2129408.0    6422528.0       2.80%    8551936.0
16        features.8  128 112 112  128 112 112          0.0       6.12      1,605,632.0      1,605,632.0    6422528.0    6422528.0       0.12%   12845056.0
17        features.9  128 112 112  128  56  56          0.0       1.53      1,204,224.0      1,605,632.0    6422528.0    1605632.0       5.63%    8028160.0
18     features.10.0  128  56  56   59  56  56       7552.0       0.71     47,181,120.0     23,683,072.0    1635840.0     740096.0       1.17%    2375936.0
19     features.10.1   59  56  56   71  56  56      37701.0       0.85    236,238,016.0    118,230,336.0     890900.0     890624.0       8.13%    1781524.0
20     features.10.2   71  56  56  256  56  56      18432.0       3.06    113,999,872.0     57,802,752.0     964352.0    3211264.0       2.18%    4175616.0
21       features.11  256  56  56  256  56  56          0.0       3.06        802,816.0        802,816.0    3211264.0    3211264.0       0.07%    6422528.0
22     features.12.0  256  56  56   81  56  56      20736.0       0.97    129,802,176.0     65,028,096.0    3294208.0    1016064.0       1.27%    4310272.0
23     features.12.1   81  56  56   78  56  56      56862.0       0.93    356,393,856.0    178,319,232.0    1243512.0     978432.0       1.22%    2221944.0
24     features.12.2   78  56  56  256  56  56      20224.0       3.06    125,239,296.0     63,422,464.0    1059328.0    3211264.0       2.10%    4270592.0
25       features.13  256  56  56  256  56  56          0.0       3.06        802,816.0        802,816.0    3211264.0    3211264.0       0.07%    6422528.0
26     features.14.0  256  56  56   73  56  56      18688.0       0.87    116,982,208.0     58,605,568.0    3286016.0     915712.0       1.21%    4201728.0
27     features.14.1   73  56  56   70  56  56      45990.0       0.84    288,229,760.0    144,224,640.0    1099672.0     878080.0       1.25%    1977752.0
28     features.14.2   70  56  56  256  56  56      18176.0       3.06    112,394,240.0     56,999,936.0     950784.0    3211264.0       2.29%    4162048.0
29       features.15  256  56  56  256  56  56          0.0       3.06        802,816.0        802,816.0    3211264.0    3211264.0       0.07%    6422528.0
30       features.16  256  56  56  256  28  28          0.0       0.77        602,112.0        802,816.0    3211264.0     802816.0       2.85%    4014080.0
31     features.17.0  256  28  28  110  28  28      28160.0       0.33     44,068,640.0     22,077,440.0     915456.0     344960.0       0.52%    1260416.0
32     features.17.1  110  28  28  131  28  28     129690.0       0.39    203,251,216.0    101,676,960.0     863720.0     410816.0       0.89%    1274536.0
33     features.17.2  131  28  28  512  28  28      67584.0       1.53    105,168,896.0     52,985,856.0     681152.0    1605632.0       1.39%    2286784.0
34       features.18  512  28  28  512  28  28          0.0       1.53        401,408.0        401,408.0    1605632.0    1605632.0       0.06%    3211264.0
35     features.19.0  512  28  28  148  28  28      75776.0       0.44    118,700,736.0     59,408,384.0    1908736.0     464128.0       1.07%    2372864.0
36     features.19.1  148  28  28  138  28  28     183816.0       0.41    288,115,296.0    144,111,744.0    1199392.0     432768.0       1.07%    1632160.0
37     features.19.2  138  28  28  512  28  28      71168.0       1.53    110,788,608.0     55,795,712.0     717440.0    1605632.0       1.12%    2323072.0
38       features.20  512  28  28  512  28  28          0.0       1.53        401,408.0        401,408.0    1605632.0    1605632.0       0.06%    3211264.0
39     features.21.0  512  28  28  132  28  28      67584.0       0.39    105,868,224.0     52,985,856.0    1875968.0     413952.0       0.97%    2289920.0
40     features.21.1  132  28  28  121  28  28     143748.0       0.36    225,302,000.0    112,698,432.0     988944.0     379456.0       1.02%    1368400.0
41     features.21.2  121  28  28  512  28  28      62464.0       1.53     97,140,736.0     48,971,776.0     629312.0    1605632.0       1.25%    2234944.0
42       features.22  512  28  28  512  28  28          0.0       1.53        401,408.0        401,408.0    1605632.0    1605632.0       0.06%    3211264.0
43       features.23  512  28  28  512  14  14          0.0       0.38        301,056.0        401,408.0    1605632.0     401408.0       1.42%    2007040.0
44     features.24.0  512  14  14  157  14  14      80384.0       0.12     31,479,756.0     15,755,264.0     722944.0     123088.0       0.51%     846032.0
45     features.24.1  157  14  14  144  14  14     203472.0       0.11     79,732,800.0     39,880,512.0     936976.0     112896.0       7.01%    1049872.0
46     features.24.2  144  14  14  512  14  14      74240.0       0.38     28,901,376.0     14,551,040.0     409856.0     401408.0       0.45%     811264.0
47       features.25  512  14  14  512  14  14          0.0       0.38        100,352.0        100,352.0     401408.0     401408.0       0.05%     802816.0
48     features.26.0  512  14  14  149  14  14      76288.0       0.11     29,875,692.0     14,952,448.0     706560.0     116816.0       0.44%     823376.0
49     features.26.1  149  14  14  147  14  14     197127.0       0.11     77,244,972.0     38,636,892.0     905324.0     115248.0       0.93%    1020572.0
50     features.26.2  147  14  14  512  14  14      75776.0       0.38     29,503,488.0     14,852,096.0     418352.0     401408.0       0.41%     819760.0
51       features.27  512  14  14  512  14  14          0.0       0.38        100,352.0        100,352.0     401408.0     401408.0       0.05%     802816.0
52     features.28.0  512  14  14  155  14  14      79360.0       0.12     31,078,740.0     15,554,560.0     718848.0     121520.0       0.45%     840368.0
53     features.28.1  155  14  14  152  14  14     212040.0       0.11     83,089,888.0     41,559,840.0     969680.0     119168.0       0.91%    1088848.0
54     features.28.2  152  14  14  512  14  14      78336.0       0.38     30,507,008.0     15,353,856.0     432512.0     401408.0       0.42%     833920.0
55       features.29  512  14  14  512  14  14          0.0       0.38        100,352.0        100,352.0     401408.0     401408.0       0.05%     802816.0
56       features.30  512  14  14  512   7   7          0.0       0.10         75,264.0        100,352.0     401408.0     100352.0       0.49%     501760.0
57           avgpool  512   7   7  512   7   7          0.0       0.10              0.0              0.0          0.0          0.0       0.22%          0.0
58      classifier.0        25088        25088          0.0       0.10              0.0              0.0          0.0          0.0       0.11%          0.0
59      classifier.1        25088         4096  102764544.0       0.02    205,516,800.0    102,760,448.0  411158528.0      16384.0       3.79%  411174912.0
60      classifier.2         4096         4096          0.0       0.02          4,096.0          4,096.0      16384.0      16384.0       0.02%      32768.0
61      classifier.3         4096         4096          0.0       0.02              0.0              0.0          0.0          0.0       0.01%          0.0
62      classifier.4         4096         4096   16781312.0       0.02     33,550,336.0     16,777,216.0   67141632.0      16384.0       0.65%   67158016.0
63      classifier.5         4096         4096          0.0       0.02          4,096.0          4,096.0      16384.0      16384.0       0.02%      32768.0
64      classifier.6         4096            2       8194.0       0.00         16,382.0          8,192.0      49160.0          8.0       0.03%      49168.0
total                                           121762674.0     138.70  5,787,169,238.0  2,921,571,420.0      49160.0          8.0     100.00%  778089680.0
===========================================================================================================================================================
Total params: 121,762,674
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total memory: 138.70MB
Total MAdd: 5.79GMAdd
Total Flops: 2.92GFlops
Total MemR+W: 742.04MB



###### AlexNet ######

Training (pretrain with imagenet):
    
    Accuracy : 0.90225
    CPU prediction time 0.01443942009456574 63
    inference time: 0:00:13.087910

    Architecture:
        AlexNetModel(
    (features): Sequential(
        (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))
        (1): ReLU(inplace=True)
        (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
        (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (4): ReLU(inplace=True)
        (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
        (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (7): ReLU(inplace=True)
        (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (9): ReLU(inplace=True)
        (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (11): ReLU(inplace=True)
        (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))
    (classifier): Sequential(
        (0): Dropout(p=0.5, inplace=False)
        (1): Linear(in_features=9216, out_features=4096, bias=True)
        (2): ReLU(inplace=True)
        (3): Dropout(p=0.5, inplace=False)
        (4): Linear(in_features=4096, out_features=4096, bias=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=4096, out_features=2, bias=True)
    )
    )

    Parameters:
        module name  input shape output shape      params memory(MB)             MAdd          Flops   MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)
0        features.0    3 224 224   64  55  55     23296.0       0.74    140,553,600.0   70,470,400.0     695296.0     774400.0      20.00%    1469696.0
1        features.1   64  55  55   64  55  55         0.0       0.74        193,600.0      193,600.0     774400.0     774400.0       0.43%    1548800.0
2        features.2   64  55  55   64  27  27         0.0       0.18        373,248.0      193,600.0     774400.0     186624.0       4.99%     961024.0
3        features.3   64  27  27  192  27  27    307392.0       0.53    447,897,600.0  224,088,768.0    1416192.0     559872.0       7.92%    1976064.0
4        features.4  192  27  27  192  27  27         0.0       0.53        139,968.0      139,968.0     559872.0     559872.0       0.19%    1119744.0
5        features.5  192  27  27  192  13  13         0.0       0.12        259,584.0      139,968.0     559872.0     129792.0       2.90%     689664.0
6        features.6  192  13  13  384  13  13    663936.0       0.25    224,280,576.0  112,205,184.0    2785536.0     259584.0      15.89%    3045120.0
7        features.7  384  13  13  384  13  13         0.0       0.25         64,896.0       64,896.0     259584.0     259584.0       0.16%     519168.0
8        features.8  384  13  13  256  13  13    884992.0       0.17    299,040,768.0  149,563,648.0    3799552.0     173056.0       2.40%    3972608.0
9        features.9  256  13  13  256  13  13         0.0       0.17         43,264.0       43,264.0     173056.0     173056.0       0.11%     346112.0
10      features.10  256  13  13  256  13  13    590080.0       0.17    199,360,512.0   99,723,520.0    2533376.0     173056.0       6.94%    2706432.0
11      features.11  256  13  13  256  13  13         0.0       0.17         43,264.0       43,264.0     173056.0     173056.0       0.32%     346112.0
12      features.12  256  13  13  256   6   6         0.0       0.04         73,728.0       43,264.0     173056.0      36864.0       0.58%     209920.0
13          avgpool  256   6   6  256   6   6         0.0       0.04              0.0            0.0          0.0          0.0       2.96%          0.0
14     classifier.0         9216         9216         0.0       0.04              0.0            0.0          0.0          0.0       0.08%          0.0
15     classifier.1         9216         4096  37752832.0       0.02     75,493,376.0   37,748,736.0  151048192.0      16384.0      23.31%  151064576.0
16     classifier.2         4096         4096         0.0       0.02          4,096.0        4,096.0      16384.0      16384.0       0.07%      32768.0
17     classifier.3         4096         4096         0.0       0.02              0.0            0.0          0.0          0.0       0.05%          0.0
18     classifier.4         4096         4096  16781312.0       0.02     33,550,336.0   16,777,216.0   67141632.0      16384.0      10.49%   67158016.0
19     classifier.5         4096         4096         0.0       0.02          4,096.0        4,096.0      16384.0      16384.0       0.07%      32768.0
20     classifier.6         4096            2      8194.0       0.00         16,382.0        8,192.0      49160.0          8.0       0.12%      49168.0
total                                          57012034.0       4.19  1,421,392,894.0  711,455,680.0      49160.0          8.0     100.00%  237247760.0
=======================================================================================================================================================
Total params: 57,012,034
-------------------------------------------------------------------------------------------------------------------------------------------------------
Total memory: 4.19MB
Total MAdd: 1.42GMAdd
Total Flops: 711.46MFlops
Total MemR+W: 226.26MB


Fine_tune (20 epochs):

    Begining:
        
        Accuracy : 0.615625
        CPU prediction time 0.00688931680104089 63
        inference time: 0:00:06.919792

        Process:
        Accuracy : 0.821875
        Accuracy : 0.826
        Accuracy : 0.832375
        Accuracy : 0.86725
        Accuracy : 0.85175
        Accuracy : 0.87925
        Accuracy : 0.866
        Accuracy : 0.8905
        Accuracy : 0.886
        Accuracy : 0.890125
        Accuracy : 0.875
        Accuracy : 0.806875
        Accuracy : 0.898375
        Accuracy : 0.854375
        Accuracy : 0.885375
        Accuracy : 0.9025
        Accuracy : 0.901125
        Accuracy : 0.89225
        Accuracy : 0.888
        Accuracy : 0.904125

    Ending:
        Accuracy : 0.904125
        CPU prediction time 0.011296151842389787 63
        inference time: 0:00:09.109909

    Architecture:
      (module): AlexNetModel(
    (features): Sequential(
      (0): Sequential(
        (0): Conv2d(3, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): Conv2d(2, 30, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2), bias=False)
        (2): Conv2d(30, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ReLU(inplace=True)
      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
      (3): Sequential(
        (0): Conv2d(64, 50, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): Conv2d(50, 71, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (2): Conv2d(71, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (4): ReLU(inplace=True)
      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
      (6): Sequential(
        (0): Conv2d(192, 75, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): Conv2d(75, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (2): Conv2d(80, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (7): ReLU(inplace=True)
      (8): Sequential(
        (0): Conv2d(384, 66, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): Conv2d(66, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (2): Conv2d(56, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (9): ReLU(inplace=True)
      (10): Sequential(
        (0): Conv2d(256, 66, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): Conv2d(66, 70, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (2): Conv2d(70, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (11): ReLU(inplace=True)
      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))
    (classifier): Sequential(
      (0): Dropout(p=0.5, inplace=False)
      (1): Linear(in_features=9216, out_features=4096, bias=True)
      (2): ReLU(inplace=True)
      (3): Dropout(p=0.5, inplace=False)
      (4): Linear(in_features=4096, out_features=4096, bias=True)
      (5): ReLU(inplace=True)
      (6): Linear(in_features=4096, out_features=2, bias=True)
    )
  )
)

    Parameters:
         module name  input shape output shape      params memory(MB)           MAdd          Flops   MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)
0       features.0.0    3 224 224    2 224 224         6.0       0.38      501,760.0      301,056.0     602136.0     401408.0       2.12%    1003544.0
1       features.0.1    2 224 224   30  55  55      7260.0       0.35   43,832,250.0   21,961,500.0     430448.0     363000.0      25.23%     793448.0
2       features.0.2   30  55  55   64  55  55      1984.0       0.74   11,616,000.0    6,001,600.0     370936.0     774400.0       1.53%    1145336.0
3         features.1   64  55  55   64  55  55         0.0       0.74      193,600.0      193,600.0     774400.0     774400.0       1.03%    1548800.0
4         features.2   64  55  55   64  27  27         0.0       0.18      373,248.0      193,600.0     774400.0     186624.0       5.97%     961024.0
5       features.3.0   64  27  27   50  27  27      3200.0       0.14    4,629,150.0    2,332,800.0     199424.0     145800.0       1.80%     345224.0
6       features.3.1   50  27  27   71  27  27     88750.0       0.20  129,345,741.0   64,698,750.0     500800.0     207036.0       7.58%     707836.0
7       features.3.2   71  27  27  192  27  27     13824.0       0.53   19,875,456.0   10,077,696.0     262332.0     559872.0       1.24%     822204.0
8         features.4  192  27  27  192  27  27         0.0       0.53      139,968.0      139,968.0     559872.0     559872.0       0.17%    1119744.0
9         features.5  192  27  27  192  13  13         0.0       0.12      259,584.0      139,968.0     559872.0     129792.0       2.81%     689664.0
10      features.6.0  192  13  13   75  13  13     14400.0       0.05    4,854,525.0    2,433,600.0     187392.0      50700.0       0.66%     238092.0
11      features.6.1   75  13  13   80  13  13     54000.0       0.05   18,238,480.0    9,126,000.0     266700.0      54080.0       1.91%     320780.0
12      features.6.2   80  13  13  384  13  13     31104.0       0.25   10,383,360.0    5,256,576.0     178496.0     259584.0       1.47%     438080.0
13        features.7  384  13  13  384  13  13         0.0       0.25       64,896.0       64,896.0     259584.0     259584.0       8.77%     519168.0
14      features.8.0  384  13  13   66  13  13     25344.0       0.04    8,555,118.0    4,283,136.0     360960.0      44616.0      12.41%     405576.0
15      features.8.1   66  13  13   56  13  13     33264.0       0.04   11,233,768.0    5,621,616.0     177672.0      37856.0       1.05%     215528.0
16      features.8.2   56  13  13  256  13  13     14592.0       0.17    4,845,568.0    2,466,048.0      96224.0     173056.0       0.40%     269280.0
17        features.9  256  13  13  256  13  13         0.0       0.17       43,264.0       43,264.0     173056.0     173056.0       0.10%     346112.0
18     features.10.0  256  13  13   66  13  13     16896.0       0.04    5,699,694.0    2,855,424.0     240640.0      44616.0       9.02%     285256.0
19     features.10.1   66  13  13   70  13  13     41580.0       0.05   14,042,210.0    7,027,020.0     210936.0      47320.0       1.50%     258256.0
20     features.10.2   70  13  13  256  13  13     18176.0       0.17    6,056,960.0    3,071,744.0     120024.0     173056.0       0.42%     293080.0
21       features.11  256  13  13  256  13  13         0.0       0.17       43,264.0       43,264.0     173056.0     173056.0       0.10%     346112.0
22       features.12  256  13  13  256   6   6         0.0       0.04       73,728.0       43,264.0     173056.0      36864.0       0.87%     209920.0
23           avgpool  256   6   6  256   6   6         0.0       0.04            0.0            0.0          0.0          0.0       1.29%          0.0
24      classifier.0         9216         9216         0.0       0.04            0.0            0.0          0.0          0.0       1.06%          0.0
25      classifier.1         9216         4096  37752832.0       0.02   75,493,376.0   37,748,736.0  151048192.0      16384.0       6.75%  151064576.0
26      classifier.2         4096         4096         0.0       0.02        4,096.0        4,096.0      16384.0      16384.0       0.07%      32768.0
27      classifier.3         4096         4096         0.0       0.02            0.0            0.0          0.0          0.0       0.04%          0.0
28      classifier.4         4096         4096  16781312.0       0.02   33,550,336.0   16,777,216.0   67141632.0      16384.0       2.40%   67158016.0
29      classifier.5         4096         4096         0.0       0.02        4,096.0        4,096.0      16384.0      16384.0       0.10%      32768.0
30      classifier.6         4096            2      8194.0       0.00       16,382.0        8,192.0      49160.0          8.0       0.13%      49168.0
total                                           54906718.0       5.52  403,969,878.0  202,918,726.0      49160.0          8.0     100.00%  231619360.0
======================================================================================================================================================
Total params: 54,906,718
------------------------------------------------------------------------------------------------------------------------------------------------------
Total memory: 5.52MB
Total MAdd: 403.97MMAdd
Total Flops: 202.92MFlops
Total MemR+W: 220.89MB




###### ResNet50 ######

training (pretrained):
    Accuracy : 0.978825
    CPU prediction time 0.2561067437368726 63
    inference time: 0:00:22.969309

    Architecture:
    ModifiedResNet50Model(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (classifier): Sequential(
    (0): Linear(in_features=2048, out_features=256, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.4, inplace=False)
    (3): Linear(in_features=256, out_features=2, bias=True)
    (4): LogSoftmax(dim=1)
  )
)

    Parameters:
                 module name   input shape  output shape      params memory(MB)             MAdd            Flops  MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)
0                      conv1     3 224 224    64 112 112      9408.0       3.06    235,225,088.0    118,013,952.0    639744.0    3211264.0       9.78%    3851008.0
1                        bn1    64 112 112    64 112 112       128.0       3.06      3,211,264.0      1,605,632.0   3211776.0    3211264.0       0.61%    6423040.0
2                       relu    64 112 112    64 112 112         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.21%    6422528.0
3                    maxpool    64 112 112    64  56  56         0.0       0.77      1,605,632.0        802,816.0   3211264.0     802816.0       6.41%    4014080.0
4             layer1.0.conv1    64  56  56    64  56  56      4096.0       0.77     25,489,408.0     12,845,056.0    819200.0     802816.0       1.80%    1622016.0
5               layer1.0.bn1    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.39%    1606144.0
6             layer1.0.conv2    64  56  56    64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       1.11%    1753088.0
7               layer1.0.bn2    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.14%    1606144.0
8             layer1.0.conv3    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       2.50%    4079616.0
9               layer1.0.bn3   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       1.23%    6424576.0
10             layer1.0.relu   256  56  56   256  56  56         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.03%    6422528.0
11     layer1.0.downsample.0    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       2.09%    4079616.0
12     layer1.0.downsample.1   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       1.54%    6424576.0
13            layer1.1.conv1   256  56  56    64  56  56     16384.0       0.77    102,559,744.0     51,380,224.0   3276800.0     802816.0       0.91%    4079616.0
14              layer1.1.bn1    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.13%    1606144.0
15            layer1.1.conv2    64  56  56    64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       0.64%    1753088.0
16              layer1.1.bn2    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.38%    1606144.0
17            layer1.1.conv3    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       1.92%    4079616.0
18              layer1.1.bn3   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       1.22%    6424576.0
19             layer1.1.relu   256  56  56   256  56  56         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.04%    6422528.0
20            layer1.2.conv1   256  56  56    64  56  56     16384.0       0.77    102,559,744.0     51,380,224.0   3276800.0     802816.0       1.15%    4079616.0
21              layer1.2.bn1    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.38%    1606144.0
22            layer1.2.conv2    64  56  56    64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       0.68%    1753088.0
23              layer1.2.bn2    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.37%    1606144.0
24            layer1.2.conv3    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       1.94%    4079616.0
25              layer1.2.bn3   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       1.19%    6424576.0
26             layer1.2.relu   256  56  56   256  56  56         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.03%    6422528.0
27            layer2.0.conv1   256  56  56   128  56  56     32768.0       1.53    205,119,488.0    102,760,448.0   3342336.0    1605632.0       2.14%    4947968.0
28              layer2.0.bn1   128  56  56   128  56  56       256.0       1.53      1,605,632.0        802,816.0   1606656.0    1605632.0       0.65%    3212288.0
29            layer2.0.conv2   128  56  56   128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0   2195456.0     401408.0       1.15%    2596864.0
30              layer2.0.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.19%     803840.0
31            layer2.0.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       1.51%    2269184.0
32              layer2.0.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.66%    3215360.0
33             layer2.0.relu   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.03%    3211264.0
34     layer2.0.downsample.0   256  56  56   512  28  28    131072.0       1.53    205,119,488.0    102,760,448.0   3735552.0    1605632.0       2.56%    5341184.0
35     layer2.0.downsample.1   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.62%    3215360.0
36            layer2.1.conv1   512  28  28   128  28  28     65536.0       0.38    102,660,096.0     51,380,224.0   1867776.0     401408.0       1.23%    2269184.0
37              layer2.1.bn1   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.09%     803840.0
38            layer2.1.conv2   128  28  28   128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0    991232.0     401408.0       0.75%    1392640.0
39              layer2.1.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.09%     803840.0
40            layer2.1.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       0.79%    2269184.0
41              layer2.1.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.68%    3215360.0
42             layer2.1.relu   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.08%    3211264.0
43            layer2.2.conv1   512  28  28   128  28  28     65536.0       0.38    102,660,096.0     51,380,224.0   1867776.0     401408.0       0.85%    2269184.0
44              layer2.2.bn1   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.09%     803840.0
45            layer2.2.conv2   128  28  28   128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0    991232.0     401408.0       0.34%    1392640.0
46              layer2.2.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.09%     803840.0
47            layer2.2.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       1.29%    2269184.0
48              layer2.2.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.69%    3215360.0
49             layer2.2.relu   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.03%    3211264.0
50            layer2.3.conv1   512  28  28   128  28  28     65536.0       0.38    102,660,096.0     51,380,224.0   1867776.0     401408.0       0.85%    2269184.0
51              layer2.3.bn1   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.08%     803840.0
52            layer2.3.conv2   128  28  28   128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0    991232.0     401408.0       0.37%    1392640.0
53              layer2.3.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.09%     803840.0
54            layer2.3.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       1.31%    2269184.0
55              layer2.3.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.67%    3215360.0
56             layer2.3.relu   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.03%    3211264.0
57            layer3.0.conv1   512  28  28   256  28  28    131072.0       0.77    205,320,192.0    102,760,448.0   2129920.0     802816.0       1.87%    2932736.0
58              layer3.0.bn1   256  28  28   256  28  28       512.0       0.77        802,816.0        401,408.0    804864.0     802816.0       0.39%    1607680.0
59            layer3.0.conv2   256  28  28   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   3162112.0     200704.0       1.21%    3362816.0
60              layer3.0.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.11%     403456.0
61            layer3.0.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.95%    2052096.0
62              layer3.0.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.15%    1613824.0
63             layer3.0.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.03%    1605632.0
64     layer3.0.downsample.0   512  28  28  1024  14  14    524288.0       0.77    205,320,192.0    102,760,448.0   3702784.0     802816.0       1.55%    4505600.0
65     layer3.0.downsample.1  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.19%    1613824.0
66            layer3.1.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       0.90%    2052096.0
67              layer3.1.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.06%     403456.0
68            layer3.1.conv2   256  14  14   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       0.64%    2760704.0
69              layer3.1.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.07%     403456.0
70            layer3.1.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.88%    2052096.0
71              layer3.1.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.14%    1613824.0
72             layer3.1.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.04%    1605632.0
73            layer3.2.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       0.89%    2052096.0
74              layer3.2.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.07%     403456.0
75            layer3.2.conv2   256  14  14   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       0.37%    2760704.0
76              layer3.2.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.07%     403456.0
77            layer3.2.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.88%    2052096.0
78              layer3.2.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.14%    1613824.0
79             layer3.2.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.04%    1605632.0
80            layer3.3.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       0.88%    2052096.0
81              layer3.3.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.06%     403456.0
82            layer3.3.conv2   256  14  14   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       0.38%    2760704.0
83              layer3.3.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.07%     403456.0
84            layer3.3.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.88%    2052096.0
85              layer3.3.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.14%    1613824.0
86             layer3.3.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.04%    1605632.0
87            layer3.4.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       0.89%    2052096.0
88              layer3.4.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.06%     403456.0
89            layer3.4.conv2   256  14  14   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       0.34%    2760704.0
90              layer3.4.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.08%     403456.0
91            layer3.4.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.87%    2052096.0
92              layer3.4.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.15%    1613824.0
93             layer3.4.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.04%    1605632.0
94            layer3.5.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       0.89%    2052096.0
95              layer3.5.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.07%     403456.0
96            layer3.5.conv2   256  14  14   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       0.38%    2760704.0
97              layer3.5.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.08%     403456.0
98            layer3.5.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.87%    2052096.0
99              layer3.5.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.15%    1613824.0
100            layer3.5.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.04%    1605632.0
101           layer4.0.conv1  1024  14  14   512  14  14    524288.0       0.38    205,420,544.0    102,760,448.0   2899968.0     401408.0       1.72%    3301376.0
102             layer4.0.bn1   512  14  14   512  14  14      1024.0       0.38        401,408.0        200,704.0    405504.0     401408.0       0.08%     806912.0
103           layer4.0.conv2   512  14  14   512   7   7   2359296.0       0.10    231,185,920.0    115,605,504.0   9838592.0     100352.0       3.00%    9938944.0
104             layer4.0.bn2   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.05%     204800.0
105           layer4.0.conv3   512   7   7  2048   7   7   1048576.0       0.38    102,660,096.0     51,380,224.0   4294656.0     401408.0       1.42%    4696064.0
106             layer4.0.bn3  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.11%     819200.0
107            layer4.0.relu  2048   7   7  2048   7   7         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.03%     802816.0
108    layer4.0.downsample.0  1024  14  14  2048   7   7   2097152.0       0.38    205,420,544.0    102,760,448.0   9191424.0     401408.0       1.56%    9592832.0
109    layer4.0.downsample.1  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.16%     819200.0
110           layer4.1.conv1  2048   7   7   512   7   7   1048576.0       0.10    102,735,360.0     51,380,224.0   4595712.0     100352.0       1.58%    4696064.0
111             layer4.1.bn1   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.08%     204800.0
112           layer4.1.conv2   512   7   7   512   7   7   2359296.0       0.10    231,185,920.0    115,605,504.0   9537536.0     100352.0       4.78%    9637888.0
113             layer4.1.bn2   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.06%     204800.0
114           layer4.1.conv3   512   7   7  2048   7   7   1048576.0       0.38    102,660,096.0     51,380,224.0   4294656.0     401408.0       1.51%    4696064.0
115             layer4.1.bn3  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.12%     819200.0
116            layer4.1.relu  2048   7   7  2048   7   7         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.03%     802816.0
117           layer4.2.conv1  2048   7   7   512   7   7   1048576.0       0.10    102,735,360.0     51,380,224.0   4595712.0     100352.0       1.57%    4696064.0
118             layer4.2.bn1   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.06%     204800.0
119           layer4.2.conv2   512   7   7   512   7   7   2359296.0       0.10    231,185,920.0    115,605,504.0   9537536.0     100352.0       1.43%    9637888.0
120             layer4.2.bn2   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.08%     204800.0
121           layer4.2.conv3   512   7   7  2048   7   7   1048576.0       0.38    102,660,096.0     51,380,224.0   4294656.0     401408.0       1.69%    4696064.0
122             layer4.2.bn3  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.14%     819200.0
123            layer4.2.relu  2048   7   7  2048   7   7         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.04%     802816.0
124                  avgpool  2048   7   7  2048   1   1         0.0       0.01              0.0              0.0         0.0          0.0       0.12%          0.0
125             classifier.0          2048           256    524544.0       0.00      1,048,320.0        524,288.0   2106368.0       1024.0       0.38%    2107392.0
126             classifier.1           256           256         0.0       0.00            256.0            256.0      1024.0       1024.0       0.02%       2048.0
127             classifier.2           256           256         0.0       0.00              0.0              0.0         0.0          0.0       0.02%          0.0
128             classifier.3           256             2       514.0       0.00          1,022.0            512.0      3080.0          8.0       0.05%       3088.0
129             classifier.4             2             2         0.0       0.00              0.0              0.0         0.0          0.0       0.32%          0.0
total                                                     24033090.0     109.69  8,216,591,870.0  4,117,014,272.0         0.0          0.0     100.00%  326753552.0
===================================================================================================================================================================
Total params: 24,033,090
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total memory: 109.69MB
Total MAdd: 8.22GMAdd
Total Flops: 4.12GFlops
Total MemR+W: 311.62MB


Fine_tune:
    Begining:
      Accuracy : 0.690875
      CPU prediction time 0.11980986973595997 63
      inference time: 0:00:11.874672

    Ending:
    Accuracy : 0.974375
    CPU prediction time 0.09621595579480367 63
    inference time: 0:00:11.482893


    Architecture:
  (module): ModifiedResNet50Model(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Sequential(
          (0): Conv2d(64, 38, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): Conv2d(38, 38, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (2): Conv2d(38, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Sequential(
          (0): Conv2d(64, 30, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): Conv2d(30, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (2): Conv2d(25, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Sequential(
          (0): Conv2d(64, 23, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): Conv2d(23, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (2): Conv2d(24, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Sequential(
          (0): Conv2d(128, 50, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): Conv2d(50, 53, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (2): Conv2d(53, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Sequential(
          (0): Conv2d(128, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): Conv2d(72, 65, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (2): Conv2d(65, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Sequential(
          (0): Conv2d(128, 55, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): Conv2d(55, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (2): Conv2d(52, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Sequential(
          (0): Conv2d(128, 46, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): Conv2d(46, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (2): Conv2d(42, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Sequential(
          (0): Conv2d(256, 106, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): Conv2d(106, 107, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (2): Conv2d(107, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Sequential(
          (0): Conv2d(256, 105, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): Conv2d(105, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Sequential(
          (0): Conv2d(256, 99, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): Conv2d(99, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (2): Conv2d(90, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Sequential(
          (0): Conv2d(256, 84, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): Conv2d(84, 79, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (2): Conv2d(79, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Sequential(
          (0): Conv2d(256, 78, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): Conv2d(78, 74, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (2): Conv2d(74, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Sequential(
          (0): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): Conv2d(80, 74, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (2): Conv2d(74, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Sequential(
          (0): Conv2d(512, 202, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): Conv2d(202, 193, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (2): Conv2d(193, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Sequential(
          (0): Conv2d(512, 187, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): Conv2d(187, 152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (2): Conv2d(152, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Sequential(
          (0): Conv2d(512, 268, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): Conv2d(268, 252, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (2): Conv2d(252, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (classifier): Sequential(
      (0): Linear(in_features=2048, out_features=256, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.4, inplace=False)
      (3): Linear(in_features=256, out_features=2, bias=True)
      (4): LogSoftmax(dim=1)
    )
  )
)
  Parameters:
                 module name   input shape  output shape      params memory(MB)             MAdd            Flops  MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)
0                      conv1     3 224 224    64 112 112      9408.0       3.06    235,225,088.0    118,013,952.0    639744.0    3211264.0       1.68%    3851008.0
1                        bn1    64 112 112    64 112 112       128.0       3.06      3,211,264.0      1,605,632.0   3211776.0    3211264.0       0.66%    6423040.0
2                       relu    64 112 112    64 112 112         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.11%    6422528.0
3                    maxpool    64 112 112    64  56  56         0.0       0.77      1,605,632.0        802,816.0   3211264.0     802816.0       4.84%    4014080.0
4             layer1.0.conv1    64  56  56    64  56  56      4096.0       0.77     25,489,408.0     12,845,056.0    819200.0     802816.0       0.40%    1622016.0
5               layer1.0.bn1    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.16%    1606144.0
6           layer1.0.conv2.0    64  56  56    38  56  56      2432.0       0.45     15,134,336.0      7,626,752.0    812544.0     476672.0       0.31%    1289216.0
7           layer1.0.conv2.1    38  56  56    38  56  56     12996.0       0.45     81,391,744.0     40,755,456.0    528656.0     476672.0       1.08%    1005328.0
8           layer1.0.conv2.2    38  56  56    64  56  56      2496.0       0.77     15,253,504.0      7,827,456.0    486656.0     802816.0       0.40%    1289472.0
9               layer1.0.bn2    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.24%    1606144.0
10            layer1.0.conv3    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       1.19%    4079616.0
11              layer1.0.bn3   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       0.55%    6424576.0
12             layer1.0.relu   256  56  56   256  56  56         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.04%    6422528.0
13     layer1.0.downsample.0    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       1.16%    4079616.0
14     layer1.0.downsample.1   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       0.53%    6424576.0
15            layer1.1.conv1   256  56  56    64  56  56     16384.0       0.77    102,559,744.0     51,380,224.0   3276800.0     802816.0       1.44%    4079616.0
16              layer1.1.bn1    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.77%    1606144.0
17          layer1.1.conv2.0    64  56  56    30  56  56      1920.0       0.36     11,948,160.0      6,021,120.0    810496.0     376320.0       0.23%    1186816.0
18          layer1.1.conv2.1    30  56  56    25  56  56      6750.0       0.30     42,257,600.0     21,168,000.0    403320.0     313600.0       1.20%     716920.0
19          layer1.1.conv2.2    25  56  56    64  56  56      1664.0       0.77     10,035,200.0      5,218,304.0    320256.0     802816.0       0.27%    1123072.0
20              layer1.1.bn2    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.15%    1606144.0
21            layer1.1.conv3    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       0.98%    4079616.0
22              layer1.1.bn3   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       0.51%    6424576.0
23             layer1.1.relu   256  56  56   256  56  56         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.03%    6422528.0
24            layer1.2.conv1   256  56  56    64  56  56     16384.0       0.77    102,559,744.0     51,380,224.0   3276800.0     802816.0       0.85%    4079616.0
25              layer1.2.bn1    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.15%    1606144.0
26          layer1.2.conv2.0    64  56  56    23  56  56      1472.0       0.28      9,160,256.0      4,616,192.0    808704.0     288512.0       0.23%    1097216.0
27          layer1.2.conv2.1    23  56  56    24  56  56      4968.0       0.29     31,084,032.0     15,579,648.0    308384.0     301056.0       1.08%     609440.0
28          layer1.2.conv2.2    24  56  56    64  56  56      1600.0       0.77      9,633,792.0      5,017,600.0    307456.0     802816.0       0.30%    1110272.0
29              layer1.2.bn2    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.16%    1606144.0
30            layer1.2.conv3    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       1.23%    4079616.0
31              layer1.2.bn3   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       0.50%    6424576.0
32             layer1.2.relu   256  56  56   256  56  56         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.03%    6422528.0
33            layer2.0.conv1   256  56  56   128  56  56     32768.0       1.53    205,119,488.0    102,760,448.0   3342336.0    1605632.0       1.46%    4947968.0
34              layer2.0.bn1   128  56  56   128  56  56       256.0       1.53      1,605,632.0        802,816.0   1606656.0    1605632.0       2.57%    3212288.0
35          layer2.0.conv2.0   128  56  56    50  56  56      6400.0       0.60     39,984,000.0     20,070,400.0   1631232.0     627200.0       3.53%    2258432.0
36          layer2.0.conv2.1    50  56  56    53  28  28     23850.0       0.16     37,355,248.0     18,698,400.0    722600.0     166208.0       2.37%     888808.0
37          layer2.0.conv2.2    53  28  28   128  28  28      6912.0       0.38     10,637,312.0      5,419,008.0    193856.0     401408.0       0.18%     595264.0
38              layer2.0.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.10%     803840.0
39            layer2.0.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       4.17%    2269184.0
40              layer2.0.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       4.35%    3215360.0
41             layer2.0.relu   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.04%    3211264.0
42     layer2.0.downsample.0   256  56  56   512  28  28    131072.0       1.53    205,119,488.0    102,760,448.0   3735552.0    1605632.0       6.23%    5341184.0
43     layer2.0.downsample.1   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.27%    3215360.0
44            layer2.1.conv1   512  28  28   128  28  28     65536.0       0.38    102,660,096.0     51,380,224.0   1867776.0     401408.0       0.77%    2269184.0
45              layer2.1.bn1   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.09%     803840.0
46          layer2.1.conv2.0   128  28  28    72  28  28      9216.0       0.22     14,394,240.0      7,225,344.0    438272.0     225792.0       0.21%     664064.0
47          layer2.1.conv2.1    72  28  28    65  28  28     42120.0       0.19     65,993,200.0     33,022,080.0    394272.0     203840.0       1.01%     598112.0
48          layer2.1.conv2.2    65  28  28   128  28  28      8448.0       0.38     13,045,760.0      6,623,232.0    237632.0     401408.0       0.22%     639040.0
49              layer2.1.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.09%     803840.0
50            layer2.1.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       0.78%    2269184.0
51              layer2.1.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.39%    3215360.0
52             layer2.1.relu   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.03%    3211264.0
53            layer2.2.conv1   512  28  28   128  28  28     65536.0       0.38    102,660,096.0     51,380,224.0   1867776.0     401408.0       0.82%    2269184.0
54              layer2.2.bn1   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.10%     803840.0
55          layer2.2.conv2.0   128  28  28    55  28  28      7040.0       0.16     10,995,600.0      5,519,360.0    429568.0     172480.0       0.20%     602048.0
56          layer2.2.conv2.1    55  28  28    52  28  28     25740.0       0.16     40,319,552.0     20,180,160.0    275440.0     163072.0       0.64%     438512.0
57          layer2.2.conv2.2    52  28  28   128  28  28      6784.0       0.38     10,436,608.0      5,318,656.0    190208.0     401408.0       0.19%     591616.0
58              layer2.2.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.10%     803840.0
59            layer2.2.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       1.09%    2269184.0
60              layer2.2.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.61%    3215360.0
61             layer2.2.relu   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.04%    3211264.0
62            layer2.3.conv1   512  28  28   128  28  28     65536.0       0.38    102,660,096.0     51,380,224.0   1867776.0     401408.0       0.88%    2269184.0
63              layer2.3.bn1   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.20%     803840.0
64          layer2.3.conv2.0   128  28  28    46  28  28      5888.0       0.14      9,196,320.0      4,616,192.0    424960.0     144256.0       0.16%     569216.0
65          layer2.3.conv2.1    46  28  28    42  28  28     17388.0       0.13     27,231,456.0     13,632,192.0    213808.0     131712.0       0.61%     345520.0
66          layer2.3.conv2.2    42  28  28   128  28  28      5504.0       0.38      8,429,568.0      4,315,136.0    153728.0     401408.0       0.27%     555136.0
67              layer2.3.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.18%     803840.0
68            layer2.3.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       1.19%    2269184.0
69              layer2.3.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.61%    3215360.0
70             layer2.3.relu   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.03%    3211264.0
71            layer3.0.conv1   512  28  28   256  28  28    131072.0       0.77    205,320,192.0    102,760,448.0   2129920.0     802816.0       1.71%    2932736.0
72              layer3.0.bn1   256  28  28   256  28  28       512.0       0.77        802,816.0        401,408.0    804864.0     802816.0       0.33%    1607680.0
73          layer3.0.conv2.0   256  28  28   106  28  28     27136.0       0.32     42,466,144.0     21,274,624.0    911360.0     332416.0       0.46%    1243776.0
74          layer3.0.conv2.1   106  28  28   107  14  14    102078.0       0.08     39,993,604.0     20,007,288.0    740728.0      83888.0       0.71%     824616.0
75          layer3.0.conv2.2   107  14  14   256  14  14     27648.0       0.19     10,737,664.0      5,419,008.0    194480.0     200704.0       0.21%     395184.0
76              layer3.0.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.08%     403456.0
77            layer3.0.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.95%    2052096.0
78              layer3.0.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.35%    1613824.0
79             layer3.0.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.03%    1605632.0
80     layer3.0.downsample.0   512  28  28  1024  14  14    524288.0       0.77    205,320,192.0    102,760,448.0   3702784.0     802816.0       1.47%    4505600.0
81     layer3.0.downsample.1  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.19%    1613824.0
82            layer3.1.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       0.87%    2052096.0
83              layer3.1.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.07%     403456.0
84          layer3.1.conv2.0   256  14  14   105  14  14     26880.0       0.08     10,516,380.0      5,268,480.0    308224.0      82320.0       0.23%     390544.0
85          layer3.1.conv2.1   105  14  14    96  14  14     90720.0       0.07     35,543,424.0     17,781,120.0    445200.0      75264.0       0.60%     520464.0
86          layer3.1.conv2.2    96  14  14   256  14  14     24832.0       0.19      9,633,792.0      4,867,072.0    174592.0     200704.0       0.19%     375296.0
87              layer3.1.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.07%     403456.0
88            layer3.1.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.80%    2052096.0
89              layer3.1.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.26%    1613824.0
90             layer3.1.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.04%    1605632.0
91            layer3.2.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       0.89%    2052096.0
92              layer3.2.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.06%     403456.0
93          layer3.2.conv2.0   256  14  14    99  14  14     25344.0       0.07      9,915,444.0      4,967,424.0    302080.0      77616.0       0.22%     379696.0
94          layer3.2.conv2.1    99  14  14    90  14  14     80190.0       0.07     31,416,840.0     15,717,240.0    398376.0      70560.0       0.54%     468936.0
95          layer3.2.conv2.2    90  14  14   256  14  14     23296.0       0.19      9,031,680.0      4,566,016.0    163744.0     200704.0       0.18%     364448.0
96              layer3.2.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.06%     403456.0
97            layer3.2.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.96%    2052096.0
98              layer3.2.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.28%    1613824.0
99             layer3.2.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.03%    1605632.0
100           layer3.3.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       0.88%    2052096.0
101             layer3.3.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.10%     403456.0
102         layer3.3.conv2.0   256  14  14    84  14  14     21504.0       0.06      8,413,104.0      4,214,784.0    286720.0      65856.0       0.20%     352576.0
103         layer3.3.conv2.1    84  14  14    79  14  14     59724.0       0.06     23,396,324.0     11,705,904.0    304752.0      61936.0       0.60%     366688.0
104         layer3.3.conv2.2    79  14  14   256  14  14     20480.0       0.19      7,927,808.0      4,014,080.0    143856.0     200704.0       0.14%     344560.0
105             layer3.3.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.09%     403456.0
106           layer3.3.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.96%    2052096.0
107             layer3.3.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.28%    1613824.0
108            layer3.3.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.03%    1605632.0
109           layer3.4.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       0.88%    2052096.0
110             layer3.4.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.10%     403456.0
111         layer3.4.conv2.0   256  14  14    78  14  14     19968.0       0.06      7,812,168.0      3,913,728.0    280576.0      61152.0       0.19%     341728.0
112         layer3.4.conv2.1    78  14  14    74  14  14     51948.0       0.06     20,349,112.0     10,181,808.0    268944.0      58016.0       0.50%     326960.0
113         layer3.4.conv2.2    74  14  14   256  14  14     19200.0       0.19      7,426,048.0      3,763,200.0    134816.0     200704.0       0.14%     335520.0
114             layer3.4.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.09%     403456.0
115           layer3.4.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.98%    2052096.0
116             layer3.4.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.28%    1613824.0
117            layer3.4.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.04%    1605632.0
118           layer3.5.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       0.88%    2052096.0
119             layer3.5.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.10%     403456.0
120         layer3.5.conv2.0   256  14  14    80  14  14     20480.0       0.06      8,012,480.0      4,014,080.0    282624.0      62720.0       0.18%     345344.0
121         layer3.5.conv2.1    80  14  14    74  14  14     53280.0       0.06     20,871,256.0     10,442,880.0    275840.0      58016.0       0.58%     333856.0
122         layer3.5.conv2.2    74  14  14   256  14  14     19200.0       0.19      7,426,048.0      3,763,200.0    134816.0     200704.0       0.17%     335520.0
123             layer3.5.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.10%     403456.0
124           layer3.5.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.96%    2052096.0
125             layer3.5.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.27%    1613824.0
126            layer3.5.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.03%    1605632.0
127           layer4.0.conv1  1024  14  14   512  14  14    524288.0       0.38    205,420,544.0    102,760,448.0   2899968.0     401408.0       1.67%    3301376.0
128             layer4.0.bn1   512  14  14   512  14  14      1024.0       0.38        401,408.0        200,704.0    405504.0     401408.0       0.16%     806912.0
129         layer4.0.conv2.0   512  14  14   202  14  14    103424.0       0.15     40,502,616.0     20,271,104.0    815104.0     158368.0       0.53%     973472.0
130         layer4.0.conv2.1   202  14  14   193   7   7    350874.0       0.04     34,376,195.0     17,192,826.0   1561864.0      37828.0       0.75%    1599692.0
131         layer4.0.conv2.2   193   7   7   512   7   7     99328.0       0.10      9,683,968.0      4,867,072.0    435140.0     100352.0       0.24%     535492.0
132             layer4.0.bn2   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.07%     204800.0
133           layer4.0.conv3   512   7   7  2048   7   7   1048576.0       0.38    102,660,096.0     51,380,224.0   4294656.0     401408.0       1.50%    4696064.0
134             layer4.0.bn3  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.11%     819200.0
135            layer4.0.relu  2048   7   7  2048   7   7         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.04%     802816.0
136    layer4.0.downsample.0  1024  14  14  2048   7   7   2097152.0       0.38    205,420,544.0    102,760,448.0   9191424.0     401408.0       1.65%    9592832.0
137    layer4.0.downsample.1  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.16%     819200.0
138           layer4.1.conv1  2048   7   7   512   7   7   1048576.0       0.10    102,735,360.0     51,380,224.0   4595712.0     100352.0       1.48%    4696064.0
139             layer4.1.bn1   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.05%     204800.0
140         layer4.1.conv2.0   512   7   7   187   7   7     95744.0       0.03      9,373,749.0      4,691,456.0    483328.0      36652.0       0.30%     519980.0
141         layer4.1.conv2.1   187   7   7   152   7   7    255816.0       0.03     25,062,520.0     12,534,984.0   1059916.0      29792.0       0.65%    1089708.0
142         layer4.1.conv2.2   152   7   7   512   7   7     78336.0       0.10      7,626,752.0      3,838,464.0    343136.0     100352.0       0.18%     443488.0
143             layer4.1.bn2   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.05%     204800.0
144           layer4.1.conv3   512   7   7  2048   7   7   1048576.0       0.38    102,660,096.0     51,380,224.0   4294656.0     401408.0       1.46%    4696064.0
145             layer4.1.bn3  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.10%     819200.0
146            layer4.1.relu  2048   7   7  2048   7   7         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.03%     802816.0
147           layer4.2.conv1  2048   7   7   512   7   7   1048576.0       0.10    102,735,360.0     51,380,224.0   4595712.0     100352.0       1.56%    4696064.0
148             layer4.2.bn1   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.07%     204800.0
149         layer4.2.conv2.0   512   7   7   268   7   7    137216.0       0.05     13,434,036.0      6,723,584.0    649216.0      52528.0       0.38%     701744.0
150         layer4.2.conv2.1   268   7   7   252   7   7    607824.0       0.05     59,554,404.0     29,783,376.0   2483824.0      49392.0       1.24%    2533216.0
151         layer4.2.conv2.2   252   7   7   512   7   7    129536.0       0.10     12,644,352.0      6,347,264.0    567536.0     100352.0       0.28%     667888.0
152             layer4.2.bn2   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.08%     204800.0
153           layer4.2.conv3   512   7   7  2048   7   7   1048576.0       0.38    102,660,096.0     51,380,224.0   4294656.0     401408.0       1.60%    4696064.0
154             layer4.2.bn3  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.09%     819200.0
155            layer4.2.relu  2048   7   7  2048   7   7         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.03%     802816.0
156                  avgpool  2048   7   7  2048   1   1         0.0       0.01              0.0              0.0         0.0          0.0       0.20%          0.0
157             classifier.0          2048           256    524544.0       0.00      1,048,320.0        524,288.0   2106368.0       1024.0       0.23%    2107392.0
158             classifier.1           256           256         0.0       0.00            256.0            256.0      1024.0       1024.0       0.01%       2048.0
159             classifier.2           256           256         0.0       0.00              0.0              0.0         0.0          0.0       0.08%          0.0
160             classifier.3           256             2       514.0       0.00          1,022.0            512.0      3080.0          8.0       0.04%       3088.0
161             classifier.4             2             2         0.0       0.00              0.0              0.0         0.0          0.0       0.11%          0.0
total                                                     15489436.0     114.95  5,555,660,982.0  2,787,928,962.0         0.0          0.0     100.00%  303622360.0
===================================================================================================================================================================
Total params: 15,489,436
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total memory: 114.95MB
Total MAdd: 5.56GMAdd
Total Flops: 2.79GFlops
Total MemR+W: 289.56MB




